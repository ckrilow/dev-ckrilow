include required(classpath("application"))

backend {
  default = "local"
  providers {
    local {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 1

        # If true submits scripts to the bash background using "&". Only
        # useful for dispatchers that do NOT submit the job and then
        # immediately return a scheduled job id.
        run-in-background = true

        # Submit string when there is no "docker" runtime attribute.
        submit = "/usr/bin/env bash ${script}"

        # File system configuration.
        filesystems {

          # For SFS backends, the "local" configuration specifies how files
          # are handled.
          local {

            # Try to hard link (ln), then soft-link (ln -s), and if both fail,
            # then copy the files.
            localization: [
              "hard-link", "soft-link", "copy"
            ]

            # Call caching strategies
            caching {
              # When copying a cached result, what type of file duplication
              # should occur. Attempted in the order listed below:
              duplication-strategy: [
                "hard-link", "soft-link", "copy"
              ]

              # Possible values: file, path, path+modtime
              # "file" will compute an md5 hash of the file content.
              # "path" will compute an md5 hash of the file path. This strategy
              # will only be effective if the duplication-strategy (above) is
              # set to "soft-link",
              # in order to allow for the original file path to be hashed.
              # "path+modtime" will compute an md5 hash of the file path and
              # the last modified time. The same conditions as for "path"
              # apply here.
              # Default: file
              hashing-strategy: "file"

              # When true, will check if a sibling file with the same name and
              # the .md5 extension exists, and if it does, use the content of
              # this file as a hash.
              # If false or the md5 does not exist, will proceed with the
              # above-defined hashing strategy.
              check-sibling-md5: false
            } # end local caching
          }
        } # end local filesystems
      } # end local config
    } # end local

    docker {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 1
        run-in-background = true
        runtime-attributes = "String docker"
        # NOTE: if no entrypoint were defined, do /bin/bash ${docker_script}
        submit-docker = """
          docker run \
            --rm -i \
            -v ${cwd}:${docker_cwd} \
            --entrypoint ${job_shell} \
            ${docker} \
            ${docker_script}
        """
        # The defaults for runtime attributes if not provided.
        default-runtime-attributes {
          docker : "ubuntu:latest"
          workflow_failure_mode : "ContinueWhilePossible"
        }
      } # end docker config
    } # end docker

    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        # concurrent-job-limit = 150

        # `script-epilogue` configures a shell command to run after the
        # execution of every command block.
        #
        # If this value is not set explicitly, the default value is `sync`,
        # equivalent to:
        # script-epilogue = "sync"
        #
        # To turn off the default `sync` behavior set this value to an empty
        # string:
        script-epilogue = ""
        #
        # Another alternative is to set a sleep between every command block
        #script-epilogue = "sleep 30"

        runtime-attributes = """
          String export = "ALL"
          String queue = "standard"
          Int cpu = 1
          Int? memory_mb
          String? slurm_account
          String? slurm_extra_param
        """
        submit = """
          sbatch \
          --job-name=${job_name} \
          --chdir=${cwd} \
          --out=${out} \
          --error=${err} \
          --export=${export} \
          --ntasks=1 \
          --ntasks-per-node=1 \
          ${true="--account=" false="" defined(slurm_account)}${slurm_account} \
          --partition=${queue} \
          ${true="--cpus-per-task=" false="" defined(cpu)}${cpu} \
          ${true="--mem-per-cpu=" false="" defined(memory_mb)}${memory_mb} \
          ${slurm_extra_param} \
          --wrap="/bin/bash ${script}"
        """
        kill = "scancel ${job_id}"
        check-alive = "squeue --jobs=${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
        # Try to hard link (ln), then soft-link (ln -s), and if both fail,
        # then copy the files.
        localization: [
          "hard-link", "soft-link", "copy"
        ]
        # Call caching strategies
        caching {
          # When copying a cached result, what type of file duplication
          # should occur. Attempted in the order listed below:
          duplication-strategy: [
            "hard-link", "soft-link", "copy"
          ]
        } # end slurm caching
        # The defaults for runtime attributes if not provided.
        default-runtime-attributes {
          workflow_failure_mode : "ContinueWhilePossible"
        }
      } # end slurm config
    } # end slurm

    sge {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        script-epilogue = "sleep 30 && sync"
        concurrent-job-limit = 100
        runtime-attributes = """
            String sge_pe = "shm"
            Int cpu = 1
            Int? gpu
            Int? time
            Int? memory_mb
            String? sge_queue
            String? sge_extra_param
        """
        submit = """
            qsub \
            -S /bin/sh \
            -terse \
            -b n \
            -N ${job_name} \
            -wd ${cwd} \
            -o ${out} \
            -e ${err} \
            ${if cpu>1 then "-pe " + sge_pe + " " else ""}${if cpu>1 then cpu else ""} \
            ${true="-l h_vmem=$(expr " false="" defined(memory_mb)}${memory_mb}${true=" / " false="" defined(memory_mb)}${if defined(memory_mb) then cpu else ""}${true=")m" false="" defined(memory_mb)} \
            ${true="-l s_vmem=$(expr " false="" defined(memory_mb)}${memory_mb}${true=" / " false="" defined(memory_mb)}${if defined(memory_mb) then cpu else ""}${true=")m" false="" defined(memory_mb)} \
            ${true="-l h_rt=" false="" defined(time)}${time}${true=":00:00" false="" defined(time)}\
            ${true="-l s_rt=" false="" defined(time)}${time}${true=":00:00" false="" defined(time)}\
            ${"-q " + sge_queue} \
            ${true="-l gpu=" false="" defined(gpu)}${gpu} \
            ${sge_extra_param} \
            -V \
            ${script}
        """
        kill = "qdel ${job_id}"
        check-alive = "qstat -j ${job_id}"
        job-id-regex = "(\\d+)"
        # The defaults for runtime attributes if not provided.
        default-runtime-attributes {
          workflow_failure_mode : "ContinueWhilePossible"
        }
      } # end sge config
    } # end sge

    google {
      # Use the Pipelines API (PAPI) version 2 by default.
      # Note: older versions of cromwell documentation will use a JES backend,
      #       "cromwell.backend.impl.jes.JesBackendLifecycleActorFactory"
      # Note: 2019: my docker image would not execute with PAPIv2;
      #       however, the pipeline works fine with JES and PAPIv1
      #actor-factory = "cromwell.backend.impl.jes.JesBackendLifecycleActorFactory"
      #actor-factory = "cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory"
      actor-factory = "cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"
      config {
        # Google project
        project = "your-project-name"

        # Base bucket for workflow executions
        root = "gs:#your-bucket-name"

        # Make the name of the backend used for call caching purposes
        # Note: insensitive to the PAPI version.
        name-for-call-caching-purposes: PAPI

        # Emit a warning if jobs last longer than this amount of time.
        # This might indicate that something got stuck in PAPI.
        slow-job-warning-time: 24 hours

        # Set various job properties
        #concurrent-job-limit = 1000
        # Set this to the lower of the two values "Queries per 100 seconds" and
        # "Queries per 100 seconds per user" for your project.
        #
        # Used to help determine maximum throughput to the Google Genomics API.
        # Setting this value too low will cause a drop in performance. Setting
        # this value too high will cause QPS based locks from Google.
        # 1000 is the default "Queries per 100 seconds per user", 50000 is the
        # default "Queries per 100 seconds"
        # See https:#cloud.google.com/genomics/quotas for more information
        genomics-api-queries-per-100-seconds = 500
        # Polling for completion backs-off gradually for slower-running jobs.
        # This is the maximum polling interval (in seconds):
        #maximum-polling-interval = 600

        # Number of workers to assign to PAPI requests
        #request-workers = 3

        # Optional Dockerhub Credentials. Can be used to access private docker
        # images.
        #dockerhub {
        #   account = ""
        #   token = ""
        #}

        genomics {
          # Set the google authentication method (e.g., application-default,
          # user-service-account). This auth is used to create pipelines and
          # manipulate auth JSONs.
          # NOTE: If combined with service account authorization, both that
          #       service account and this service account must be able to
          #       read and write to the 'root' GCS path
          auth = "user-service-account"
          # Endpoint for APIs, no reason to change this unless directed by
          # Google.
          endpoint-url = "https:#genomics.googleapis.com/"
          # Allows you to use an alternative service account to launch jobs,
          # by default uses default service account
          compute-service-account = "default"
          # Pipelines v2 only: specify the number of times localization and
          # delocalization operations should be attempted. There is no logic
          # to determine if the error was transient or not, everything is
          # retried upon failure.
          #localization-attempts = 3
          # Restrict access to VM metadata. Useful in cases when untrusted
          # containers are running under a service account not owned by the
          # submitting user
          #restrict-metadata-access = false
        }

        filesystems {
          gcs {
            # Set the google authentication method (e.g., application-default,
            # user-service-account).
            auth = "user-service-account"

            caching {
              # When a cache hit is found, the following duplication strategy
              # will be followed to use the cached outputs
              # Possible values: "copy", "reference". Defaults to "copy"
              # "copy": Copy the output files
              # "reference":
              #   DO NOT copy the output files but point to the original output
              #   files instead. Will still make sure that all the original
              #   output files exist and are accessible before going forward
              #   with the cache hit.
              duplication-strategy = "reference"
            }
          }
        }

        # The defaults for runtime attributes if not provided.
        default-runtime-attributes {
          docker : "ubuntu:latest"
          workflow_failure_mode : "ContinueWhilePossible"
        }
      } # end google config
    } # end google
  }
}


services {
  LoadController {
    class = "cromwell.services.loadcontroller.impl.LoadControllerServiceActor"
    config {
      control-frequency = 21474834 seconds
    }
  }
}


system {
  abort-jobs-on-terminate = true

  # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to
  # gracefully shutdown in server mode, in particular clearing up all queued
  # database writes before letting the JVM shut down. The shutdown is a
  # multi-phase process, each phase having its own configurable timeout.
  graceful-server-shutdown = true

  # If 'true' then when Cromwell starts up, it tries to restart
  # incomplete workflows
  workflow-restart = false

  # Cromwell will cap the number of running workflows at N
  max-concurrent-workflows = 1000000

  # Cromwell will launch up to N submitted workflows at a time, regardless of
  # how many open workflow slots exist
  max-workflow-launch-count = 1000000

  # Number of seconds between workflow launches
  new-workflow-poll-rate = 20

  # Since the WorkflowLogCopyRouter is initialized in code, this is the number
  # of workers
  number-of-workflow-log-copy-workers = 10

  # Default number of cache read workers
  number-of-cache-read-workers = 25

  # Maximum scatter width per scatter node. Cromwell will fail the workflow
  # if the scatter width goes beyond N
  max-scatter-width-per-scatter = 10000000

  # Total max. jobs that can be created per root workflow. If it goes beyond N,
  # Cromwell will fail the workflow by:
  # - no longer creating new jobs
  # - let the jobs that have already been started finish, and then fail the
  #   workflow
  total-max-jobs-per-root-workflow = 10000000

  # Rate at which Cromwell updates its instrumentation gauge metrics (e.g:
  # Number of workflows running, queued, etc..)
  instrumentation-rate = 5 seconds

  job-rate-control {
    jobs = 500
    per = 1 second
  }

  workflow-heartbeats {
    heartbeat-interval: 2 minutes
    ttl: 10 minutes
    write-failure-shutdown-duration: 5 minutes
    write-batch-size: 10000
    write-threshold: 10000
  }

}


call-caching {
  enabled = false
  invalidate-bad-cache-results = true
}


# Set general authentication methods for google
google {
  application-name = "cromwell"
  auths = [
    {
      # Use gsutil default authentication
      # NOTE: use service account via `gcloud auth activate-service-account`
      name = "application-default"
      scheme = "application_default"
    },
    {
      # Use a user service account: recommended
      # NOTE: must provide key to google.json::user_service_account_json
      name = "user-service-account"
      scheme = "user_service_account"
    }
  ]
}
