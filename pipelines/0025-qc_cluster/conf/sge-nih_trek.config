// Configuration for Sun Grid Engine (SGE) workload management platform

executor {
    name = 'sge'

    // Set perJobMemLimit to true. See:
    // * https://github.com/nextflow-io/nextflow/issues/123
    // * https://gitter.im/nextflow-io/nextflow/archives/2018/02/09
    perJobMemLimit = true
} // end executor

process {
    executor = 'sge'

    // sge users will need to edit the below parameters to fit their platform
    queueSize = 1000
    killBatchSize = 1000
    clusterOptions = { "-l h_vmem="+task.memory.toMega()+"M" }
    penv = "make-dedicated"

    withLabel: gpu {
        //queue = 'gpu_testing'
        // need to use trek non-standard queue
        clusterOptions = { "-l h_vmem="+task.memory.toMega()+"M,gpu=1 -P 'gpu_testing'" }
    }

    // BASE SETTINGS START
    // process-specific resources
    // These are here because trek submits memory differently -- you specify
    // memory PER CPU. To avoid hogging resources, we have to divide the
    // memory we want by # of cpus
    withName: normalize_and_pca {
        memory = { (50.GB / task.cpus) * task.attempt }
    }
    withName: bbknn {
        memory = { (20.GB / task.cpus) * task.attempt }
    }
    withName: cluster {
        memory = { (20.GB / task.cpus) * task.attempt }
    }
    withName: cluster_validate_resolution_sklearn {
        memory = { (40.GB / task.cpus * task.attempt) * task.attempt }
    }
    withName: cluster_markers {
        memory = { (15.GB / task.cpus) * task.attempt }
    }
    withName: merge_clusters {
        memory = { (30.GB / task.cpus) * task.attempt }
    }
    withName: umap_calculate {
        memory = { (50.GB / task.cpus) * task.attempt }
    }
    withName: umap_gather {
        memory = { 400.GB * task.attempt }
    }
    withName: umap_calculate_and_plot {
        memory = { (50.GB / task.cpus) * task.attempt }
    }
    // BASE SETTINGS END

} // end process

timeline {
    enabled = true
}
